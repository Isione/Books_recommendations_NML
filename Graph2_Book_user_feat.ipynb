{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score, balanced_accuracy_score\n",
    "import wandb\n",
    "import datetime\n",
    "import os\n",
    "import yaml\n",
    "import logging\n",
    "import h5py"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b77f67c9f2cfa61",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Configure logger\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a894cfb252c8de4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Loading data and processing data"
   ],
   "id": "52093d4b0c011ac8"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Load csv file ratings, books_clean and user_fav_genres\n",
    "users_2_books = pd.read_csv('data/ratings.csv')\n",
    "books_2_genres = pd.read_csv('data/books_clean.csv', converters={\"genres\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").replace(\" \",\"\")})\n",
    "users_2_genres = pd.read_csv('data/user_genres.csv', converters={\"genres\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").replace(\" \",\"\")})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Book features from genres"
   ],
   "id": "57f076d117019a5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "book_genres = books_2_genres['genres'].str.get_dummies(\",\")\n",
    "logger.info(f\"Book genres: {book_genres.columns}\")"
   ],
   "id": "98b7592d3088d816",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "book_feat = torch.from_numpy(book_genres.values).to(torch.float)\n",
    "assert book_feat.size() == (10000, 39)  "
   ],
   "id": "ef8fdefa3e641621",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## User features from genres"
   ],
   "id": "5f8483631b4f95de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "unique_user_id = users_2_books['user_id'].unique()\n",
    "unique_user_id = np.sort(unique_user_id)\n",
    "\n",
    "# Now we want to make sure that all the user id are in the user_2_genres\n",
    "# We will add the missing user id with all genres as []\n",
    "counter = 0\n",
    "for user_id in unique_user_id:\n",
    "    if user_id not in users_2_genres[\"user_id\"].values:\n",
    "        counter += 1\n",
    "        df_to_append = pd.DataFrame([{\"user_id\": user_id, \"genres\": \"\"}])\n",
    "        users_2_genres = pd.concat([users_2_genres, df_to_append], ignore_index=True)\n",
    "\n",
    "logger.info(f\"Number of users added: {counter}\")"
   ],
   "id": "50b3226a5ece34ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "user_genres = users_2_genres['genres'].str.get_dummies(\",\")\n",
    "\n",
    "# Add the missing columns to the user_genres to have the same number of columns on user_genres and book_genres\n",
    "for column in book_genres.columns:\n",
    "    if column not in user_genres.columns:\n",
    "        user_genres[column] = 0\n",
    "        \n",
    "logger.info(f\"User genres: {user_genres.columns}\")\n",
    "\n",
    "user_genres.head()\n",
    "user_feat = torch.from_numpy(user_genres.values).to(torch.float)"
   ],
   "id": "9e317d0fe9cb8b97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Edge Index and mapping between user and book"
   ],
   "id": "94e2bbb7096b099f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a mapping from unique user indices to range [0, num_user_nodes):\n",
    "unique_user_id = users_2_books['user_id'].unique()\n",
    "\n",
    "unique_user_id = np.sort(unique_user_id)\n",
    "unique_user_id = pd.DataFrame(data={\n",
    "    'user_id': unique_user_id,\n",
    "    'mapped_id': pd.RangeIndex(len(unique_user_id)),\n",
    "})\n",
    "\n",
    "logger.info(f\"Mapping of user IDs to consecutive values: \\n {unique_user_id.head()}\")\n"
   ],
   "id": "52053be9347b69d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a mapping from unique book indices to range [0, num_book_nodes):\n",
    "unique_book_id = users_2_books['book_id'].unique()\n",
    "unique_book_id = np.sort(unique_book_id)\n",
    "unique_book_id = pd.DataFrame(data={\n",
    "    'book_id': unique_book_id,\n",
    "    'mapped_id': pd.RangeIndex(len(unique_book_id)),\n",
    "})\n",
    "\n",
    "logger.info(f\"Mapping of book IDs to consecutive values: \\n {unique_book_id.head()}\")"
   ],
   "id": "d1a12e17d3b0b113",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Perform merge to obtain the edges from users and books:\n",
    "ratings_user_id = pd.merge(users_2_books['user_id'], unique_user_id,\n",
    "                            left_on='user_id', right_on='user_id', how='left')\n",
    "ratings_user_id = torch.from_numpy(ratings_user_id['mapped_id'].values)\n",
    "ratings_book_id = pd.merge(users_2_books['book_id'], unique_book_id,\n",
    "                            left_on='book_id', right_on='book_id', how='left')\n",
    "ratings_book_id = torch.from_numpy(ratings_book_id['mapped_id'].values)\n",
    "\n",
    "edge_index_user_to_book = torch.stack([ratings_user_id, ratings_book_id], dim=0)\n",
    "\n",
    "logger.info(f\"Final edge indices pointing from users to books: \\n {edge_index_user_to_book}\")         "
   ],
   "id": "3c8bd53cf3d53fd0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Hetero data initialization"
   ],
   "id": "134e70934ca59d64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = HeteroData()\n",
    "\n",
    "# Save node indices:\n",
    "data[\"user\"].node_id = torch.arange(len(unique_user_id))\n",
    "data[\"book\"].node_id = torch.arange(len(books_2_genres))\n",
    "\n",
    "# Add the node features and edge indices:\n",
    "data[\"book\"].x = book_feat\n",
    "data[\"user\"].x = user_feat\n",
    "\n",
    "data[\"user\", \"rates\", \"book\"].edge_index = edge_index_user_to_book\n",
    "\n",
    "# `T.ToUndirected()` makes sure to add the reverse edges from books to users to let the GNN pass messages in both directions.\n",
    "data = T.ToUndirected()(data)"
   ],
   "id": "bab229da276c01d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Functions"
   ],
   "id": "50759bf3fa6f1fb9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "    def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "    def forward(self, x_user: Tensor, x_book: Tensor, edge_label_index: Tensor) -> Tensor:\n",
    "        # Convert node embeddings to edge-level representations:\n",
    "        edge_feat_user = x_user[edge_label_index[0]]\n",
    "        edge_feat_book = x_book[edge_label_index[1]]\n",
    "        # Apply dot-product to get a prediction \n",
    "        return (edge_feat_user * edge_feat_book).sum(dim=-1)\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        # Since the dataset does not come with rich features, we also learn two\n",
    "        # embedding matrices for users and movies:\n",
    "        self.book_lin = torch.nn.Linear(39, hidden_channels)\n",
    "        self.user_lin = torch.nn.Linear(39, hidden_channels)\n",
    "        self.user_emb = torch.nn.Embedding(data[\"user\"].num_nodes, hidden_channels)\n",
    "        self.book_emb = torch.nn.Embedding(data[\"book\"].num_nodes, hidden_channels)\n",
    "        # Instantiate homogeneous GNN:\n",
    "        self.gnn = GNN(hidden_channels)\n",
    "\n",
    "        # Convert GNN model into a heterogeneous:\n",
    "        self.gnn = to_hetero(self.gnn, metadata=data.metadata())\n",
    "        self.classifier = Classifier()\n",
    "        \n",
    "    def forward(self, data: HeteroData) -> Tensor:\n",
    "        x_dict = {\n",
    "          \"user\": self.user_lin(data[\"user\"].x) + self.user_emb(data[\"user\"].node_id),\n",
    "          \"book\": self.book_lin(data[\"book\"].x) + self.book_emb(data[\"book\"].node_id),\n",
    "        } \n",
    "        \n",
    "        # `x_dict` holds feature matrices of all node types\n",
    "        # `edge_index_dict` holds all edge indices of all edge types\n",
    "        x_dict = self.gnn(x_dict, data.edge_index_dict)\n",
    "        pred = self.classifier(\n",
    "            x_dict[\"user\"],\n",
    "            x_dict[\"book\"],\n",
    "            data[\"user\", \"rates\", \"book\"].edge_label_index,\n",
    "        )\n",
    "        return pred\n",
    "        \n"
   ],
   "id": "1fd6fc96d188a6ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def validate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    ground_truths = []\n",
    "    for sampled_data in tqdm.tqdm(val_loader):\n",
    "        with torch.no_grad():\n",
    "            sampled_data.to(device)\n",
    "            preds.append(model(sampled_data))\n",
    "            ground_truths.append(sampled_data[\"user\", \"rates\", \"book\"].edge_label)            \n",
    "    pred = torch.cat(preds, dim=0).cpu().numpy()\n",
    "    ground_truth = torch.cat(ground_truths, dim=0).cpu().numpy()\n",
    "    auc = roc_auc_score(ground_truth, pred)\n",
    "    acc = balanced_accuracy_score(ground_truth, [sigmoid(pred[i]) >= 0.5 for i in range(len(pred))])\n",
    "    \n",
    "    return auc, acc\n",
    "\n",
    "def sigmoid(x):                                        \n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def save_hetero_data(data: HeteroData, file_path: str) -> None:\n",
    "    with h5py.File(file_path, 'w') as f:\n",
    "        for node_type in data.node_types:\n",
    "            if 'x' in data[node_type]:\n",
    "                f.create_dataset(f'{node_type}/x', data=data[node_type].x.cpu().numpy())\n",
    "        for edge_type in data.edge_types:\n",
    "            f.create_dataset(f'{edge_type}/edge_index', data=data[edge_type].edge_index.cpu().numpy())\n",
    "            if 'edge_label' in data[edge_type]:\n",
    "                f.create_dataset(f'{edge_type}/edge_label', data=data[edge_type].edge_label.cpu().numpy())\n",
    "                \n",
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    ground_truths = []\n",
    "    for sampled_data in tqdm.tqdm(test_loader):\n",
    "        with torch.no_grad():\n",
    "            sampled_data.to(device)\n",
    "            preds.append(model(sampled_data))\n",
    "            ground_truths.append(sampled_data[\"user\", \"rates\", \"book\"].edge_label)            \n",
    "    pred = torch.cat(preds, dim=0).cpu().numpy()\n",
    "    ground_truth = torch.cat(ground_truths, dim=0).cpu().numpy()\n",
    "    auc = roc_auc_score(ground_truth, pred)\n",
    "    acc = balanced_accuracy_score(ground_truth, [sigmoid(pred[i]) >= 0.5 for i in range(len(pred))])\n",
    "    \n",
    "    return auc, acc"
   ],
   "id": "7d67a6808cf5a90",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Wandb Sweep"
   ],
   "id": "eb0df6b1fb6db3cc"
  },
  {
   "cell_type": "code",
   "source": [
    "#read the .env file\n",
    "wandb_token = os.getenv(\"NML_ACCESS_TOKEN\")\n",
    "wandb.login(key=wandb_token)\n",
    "\n",
    "# Initialize wandb\n",
    "wandb_project_name = 'Final_sweep_Book_user_feat'\n",
    "date_time = datetime.datetime.now().strftime(\"%m_%d_%H_%M_%S\")\n",
    "day_time = datetime.datetime.now().strftime(\"%m_%d\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "849ba656345a69d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def upgrade_file_version(folder_path: str) -> int:\n",
    "    new_version = 1\n",
    "    for f in os.listdir(folder_path):\n",
    "        if f.startswith(f'{date_time}'):\n",
    "            file_path = os.path.join(folder_path, f) \n",
    "            version = file_path.split(\"_\")[-1].split(\".\")[0][1:]\n",
    "            new_version = int(version) + 1\n",
    "    return new_version\n",
    "# read sweep_config.yaml file\n",
    "with open(\"sweep_config.yaml\", 'r') as stream:\n",
    "    sweep_config = yaml.safe_load(stream)\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=wandb_project_name)\n",
    "\n",
    "logger.info(f\"Sweep config: {sweep_config}\")\n",
    "logger.info(f\"Sweep id: {sweep_id}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42970bddcccbbf1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train(config=None):\n",
    "    \n",
    "    with wandb.init(config=config):\n",
    "        \n",
    "        config = wandb.config\n",
    "        \n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        logger.info(f\"Device: {device}\")\n",
    "    \n",
    "        # Create splits data\n",
    "        transform = T.RandomLinkSplit(\n",
    "            num_val=0.1,\n",
    "            num_test=0.1,\n",
    "            disjoint_train_ratio=0.3,\n",
    "            neg_sampling_ratio=config.negative_sampling_ratio,\n",
    "            add_negative_train_samples=False,\n",
    "            edge_types=(\"user\", \"rates\", \"book\"),\n",
    "            rev_edge_types=(\"book\", \"rev_rates\", \"user\"), \n",
    "        )\n",
    "        train_data, val_data, test_data = transform(data)\n",
    "        \n",
    "        # Create loaders\n",
    "        edge_label_index = train_data[\"user\", \"rates\", \"book\"].edge_label_index\n",
    "        edge_label = train_data[\"user\", \"rates\", \"book\"].edge_label\n",
    "        train_loader = LinkNeighborLoader(\n",
    "            data=train_data,\n",
    "            num_neighbors=[config.first_num_neighbours, config.second_num_neighbours],\n",
    "            neg_sampling_ratio=config.negative_sampling_ratio,\n",
    "            edge_label_index=((\"user\", \"rates\", \"book\"), edge_label_index),\n",
    "            edge_label=edge_label,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        edge_label_index = val_data[\"user\", \"rates\", \"book\"].edge_label_index\n",
    "        edge_label = val_data[\"user\", \"rates\", \"book\"].edge_label\n",
    "        val_loader = LinkNeighborLoader(\n",
    "            data=val_data,\n",
    "            num_neighbors=[config.first_num_neighbours, config.second_num_neighbours],\n",
    "            edge_label_index=((\"user\", \"rates\", \"book\"), edge_label_index),\n",
    "            edge_label=edge_label,\n",
    "            batch_size=(1+config.negative_sampling_ratio) * config.batch_size,\n",
    "            shuffle=False,\n",
    "        )\n",
    "        \n",
    "        edge_label_index = test_data[\"user\", \"rates\", \"book\"].edge_label_index\n",
    "        edge_label = test_data[\"user\", \"rates\", \"book\"].edge_label\n",
    "        test_loader = LinkNeighborLoader(\n",
    "            data=test_data,\n",
    "            num_neighbors=[config.first_num_neighbours, config.second_num_neighbours],\n",
    "            edge_label_index=((\"user\", \"rates\", \"book\"), edge_label_index),\n",
    "            edge_label=edge_label,\n",
    "            batch_size=(1+config.negative_sampling_ratio) * config.batch_size,\n",
    "            shuffle=False,\n",
    "        )\n",
    "        \n",
    "        model = Model(hidden_channels=config.hidden_channels)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        if config.optimizer == \"Adam\":\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "        elif config.optimizer == \"SGD\": \n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=config.lr)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid optimizer\")\n",
    "        \n",
    "        \n",
    "        if not os.path.exists(f'models/book_user_feat/{day_time}_v000'):\n",
    "            version = 0\n",
    "        else: \n",
    "            folder_path_start = f'models/book_user_feat'\n",
    "            version = upgrade_file_version(folder_path_start) \n",
    "\n",
    "        os.makedirs(f'models/book_user_feat/{day_time}_v{int(version):03d}')\n",
    "        saving_path = f'models/book_user_feat/{day_time}_v{int(version):03d}'\n",
    "        logger.info(f\"Saving path: {saving_path}\")\n",
    "        \n",
    "        #save test splits for future testings on the best model\n",
    "        test_set_path = saving_path + '/test_data.h5'\n",
    "        save_hetero_data(train_data, test_set_path)\n",
    "        logger.info(f\"Test data saved at: {test_set_path}\")\n",
    "        \n",
    "        metrics_dict = {}\n",
    "        \n",
    "        for epoch in range(0, config.epochs):\n",
    "            #Training \n",
    "            total_loss = total_examples = 0\n",
    "            pred_list = []\n",
    "            gt_list = []\n",
    "            for sampled_data in tqdm.tqdm(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                sampled_data.to(device)\n",
    "                pred = model(sampled_data)\n",
    "                ground_truth = sampled_data[\"user\", \"rates\", \"book\"].edge_label\n",
    "                if config.loss == \"CrossEntropyLoss\":\n",
    "                    loss = F.binary_cross_entropy_with_logits(pred, ground_truth)\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid loss\")\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += float(loss) * pred.numel()\n",
    "                total_examples += pred.numel()\n",
    "                pred_list.extend(torch.sigmoid(pred).detach().cpu().numpy())\n",
    "                gt_list.extend(ground_truth.cpu().numpy())\n",
    "                \n",
    "            loss = total_loss / total_examples\n",
    "            acc_train = balanced_accuracy_score(gt_list, [pred_list[i] >= 0.5 for i in range(len(pred_list))])\n",
    "        \n",
    "            torch.save(model.state_dict(), saving_path + f'/model_epoch_{epoch}.pth')\n",
    "            \n",
    "            # Validatation\n",
    "            val_auc, acc_val = validate(model, val_loader, device)\n",
    "            \n",
    "            # Log the validation AUC to wandb\n",
    "            wandb.log({'epoch': epoch, 'Validation AUC': val_auc, 'Loss': loss ,'training accuracy': acc_train, 'validation_accuracy': acc_val})\n",
    "            logger.info(f\"Epoch {epoch} - Validation AUC: {val_auc:.4f} - Loss: {loss:.4f} - Training Accuracy: {acc_train:.4f} - Validation Accuracy: {acc_val:.4f}\")\n",
    "            \n",
    "            metrics_dict[epoch] = {\"loss\": loss, \"auc\": val_auc}\n",
    "            \n",
    "            #save metrics with pickle\n",
    "            with open(saving_path + '/metrics_dict.pkl', 'wb') as f:\n",
    "                pickle.dump(metrics_dict, f)\n",
    "        \n",
    "        torch.save(model.state_dict(), saving_path + f'/final_model.pth')\n",
    "        \n",
    "        # Test the model\n",
    "        with torch.no_grad():\n",
    "            test_auc, acc_test = test(model, test_loader, device)\n",
    "        \n",
    "        logger.info(f\"Test AUC: {test_auc:.4f} - Test Accuracy: {acc_test:.4f}\")\n",
    "        \n",
    "        # Finish the wandb run\n",
    "        wandb.finish()\n",
    "            "
   ],
   "id": "ea8252761f388bba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "wandb.agent(sweep_id, train, count=20)",
   "id": "f67d7b84dec0e418",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [],
   "id": "51ba06ac5e70eb85"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
